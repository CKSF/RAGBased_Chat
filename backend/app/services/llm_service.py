from openai import OpenAI
from backend.config import Config
from typing import List, Optional, Generator

class LLMService:
    def __init__(self):
        """Initialize the Volcengine client using OpenAI SDK."""
        self.client = OpenAI(
            api_key=Config.VOLC_API_KEY,
            base_url=Config.VOLC_BASE_URL
        )
        self.model = Config.VOLC_MODEL
        self.lite_model = Config.VOLC_LITE_MODEL  # Fast model for simple tasks
        
    def stream_response(self, user_prompt: str, system_prompt: str = None, history: List[dict] = None) -> Generator[str, None, None]:
        """
        Stream response token by token.
        """
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            # Sanitize history: Volcengine strict mode only allows 'role' and 'content'
            clean_history = [{"role": m["role"], "content": m["content"]} for m in history if m["role"] in ["user", "assistant"]]
            messages.extend(clean_history)
            
        messages.append({"role": "user", "content": user_prompt})
        
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                temperature=0.7 
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"[LLM ERROR: {str(e)}]"

    def get_response(self, user_prompt: str, system_prompt: str = None, history: List[dict] = None) -> str:
        """
        Get a simple response from the LLM.
        """
        messages = []
        
        # 1. Add System Prompt
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
            
        # 2. Add History (Context)
        if history:
            messages.extend(history)
            
        # 3. Add Current User Prompt
        messages.append({"role": "user", "content": user_prompt})
        
        try:
            print(f"ðŸ¤– Calling Volcengine API [{self.model}]...")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                # temperature=0.7, 
            )
            
            # Extract content
            answer = completion.choices[0].message.content
            return answer
            
        except Exception as e:
            print(f"âŒ LLM API Error: {e}")
            return f"Error calling LLM: {str(e)}"
    
    def rewrite_query(self, user_query: str, history: List[dict]) -> str:
        """
        Rewrite user query based on conversation history to make it self-contained.
        Uses lightweight model for speed.
        
        Args:
            user_query: Original user question
            history: List of previous messages [{"role": "user/assistant", "content": "..."}]
            
        Returns:
            Rewritten query (or original if no rewriting needed)
        """
        # Quick check: Does query need rewriting?
        needs_rewrite = self._should_rewrite(user_query, history)
        
        if not needs_rewrite:
            print(f"â„¹ï¸  Query is complete, no rewriting needed: {user_query}")
            return user_query
        
        # Extract recent context (last 2 turns = 4 messages)
        recent_turns = history[-4:] if len(history) >= 4 else history
        
        if not recent_turns:
            return user_query
        
        # Build context string (truncate long messages)
        context_lines = []
        for msg in recent_turns:
            role = "ç”¨æˆ·" if msg['role'] == 'user' else "åŠ©æ‰‹"
            content = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
            context_lines.append(f"{role}: {content}")
        context_str = "\n".join(context_lines)
        
        # Construct rewrite prompt
        rewrite_prompt = f"""åŸºäºŽä»¥ä¸‹å¯¹è¯åŽ†å²ï¼Œå°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªç‹¬ç«‹å®Œæ•´çš„æ£€ç´¢æŸ¥è¯¢ã€‚

è¦æ±‚ï¼š
1. å¦‚æžœé—®é¢˜å·²ç»å®Œæ•´ç‹¬ç«‹ï¼Œç›´æŽ¥è¿”å›žåŽŸé—®é¢˜
2. å¦‚æžœé—®é¢˜ä¾èµ–ä¸Šæ–‡ï¼ˆå¦‚ä½¿ç”¨"å®ƒ"ã€"è¿™ä¸ª"ã€"é‚£ä¸ª"ã€"å‘¢"ã€"è¿˜æœ‰"ç­‰æŒ‡ä»£è¯ï¼‰ï¼Œè¡¥å……ä¸»è¯­å’ŒèƒŒæ™¯
3. ä¿æŒåŽŸé—®é¢˜çš„æ ¸å¿ƒæ„å›¾
4. è¾“å‡ºï¼šç›´æŽ¥è¿”å›žæ”¹å†™åŽçš„æŸ¥è¯¢ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–æ ‡ç‚¹

å¯¹è¯åŽ†å²ï¼š
{context_str}

ç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š{user_query}

æ”¹å†™åŽçš„ç‹¬ç«‹æŸ¥è¯¢ï¼š"""

        try:
            print(f"ðŸ”„ Query rewriting: {user_query}")
            
            # Use lightweight model for fast rewriting
            completion = self.client.chat.completions.create(
                model=self.lite_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æŸ¥è¯¢æ”¹å†™ä¸“å®¶ã€‚å°†ä¸å®Œæ•´çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹å®Œæ•´çš„æ£€ç´¢æŸ¥è¯¢ã€‚"},
                    {"role": "user", "content": rewrite_prompt}
                ],
                max_tokens=100,  # Limit output length
                temperature=0.3   # Low temperature for consistent rewrites
            )
            
            rewritten = completion.choices[0].message.content.strip()
            print(f"âœ… Rewritten query: {rewritten}")
            return rewritten
            
        except Exception as e:
            print(f"âŒ Query rewrite failed: {e}, using original query")
            return user_query
    
    def _should_rewrite(self, query: str, history: List[dict]) -> bool:
        """
        Determine if query needs rewriting based on simple heuristics.
        
        Returns:
            True if query appears to depend on context
        """
        if not history:
            return False
        
        # Indicators that query is incomplete or context-dependent
        indicators = [
            len(query) < 15,  # Very short query
            any(word in query for word in ["å®ƒ", "è¿™ä¸ª", "é‚£ä¸ª", "å‘¢", "è¿˜æœ‰", "è¿™", "é‚£"]),  # Pronouns/references
            query.strip().endswith("ï¼Ÿ") and len(query) < 10,  # Short question
            any(phrase in query for phrase in ["è¯¦ç»†", "ä¸¾ä¾‹", "ä¸ºä»€ä¹ˆ", "æ€Žä¹ˆ", "å¦‚ä½•"])  # Follow-up keywords
        ]
        
        return any(indicators)

