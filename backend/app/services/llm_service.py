from openai import OpenAI
from backend.config import Config
from typing import List, Optional, Generator

class LLMService:
    def __init__(self):
        """Initialize the Volcengine client using OpenAI SDK."""
        self.client = OpenAI(
            api_key=Config.VOLC_API_KEY,
            base_url=Config.VOLC_BASE_URL
        )
        self.model = Config.VOLC_MODEL
        self.lite_model = Config.VOLC_LITE_MODEL  # Fast model for simple tasks
        
    def stream_response(self, user_prompt: str, system_prompt: str = None, history: List[dict] = None) -> Generator[str, None, None]:
        """
        Stream response token by token.
        """
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            # Sanitize history: Volcengine strict mode only allows 'role' and 'content'
            clean_history = [{"role": m["role"], "content": m["content"]} for m in history if m["role"] in ["user", "assistant"]]
            messages.extend(clean_history)
            
        messages.append({"role": "user", "content": user_prompt})
        
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                temperature=0.2 
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"[LLM ERROR: {str(e)}]"

    def get_response(self, user_prompt: str, system_prompt: str = None, history: List[dict] = None) -> str:
        """
        Get a simple response from the LLM.
        """
        messages = []
        
        # 1. Add System Prompt
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
            
        # 2. Add History (Context)
        if history:
            messages.extend(history)
            
        # 3. Add Current User Prompt
        messages.append({"role": "user", "content": user_prompt})
        
        try:
            print(f"ðŸ¤– Calling Volcengine API [{self.model}]...")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2, 
            )
            
            # Extract content
            answer = completion.choices[0].message.content
            return answer
            
        except Exception as e:
            print(f"âŒ LLM API Error: {e}")
            return f"Error calling LLM: {str(e)}"
    
    def rewrite_query(self, user_query: str, history: List[dict]) -> str:
        """
        Rewrite user query based on conversation history to make it self-contained.
        Uses lightweight model for speed.
        
        Args:
            user_query: Original user question
            history: List of previous messages [{"role": "user/assistant", "content": "..."}]
            
        Returns:
            Rewritten query (or original if no rewriting needed)
        """
        # Quick check: Does query need rewriting?
        needs_rewrite = self._should_rewrite(user_query, history)
        
        if not needs_rewrite:
            print(f"â„¹ï¸  Query is complete, no rewriting needed: {user_query}")
            return user_query
        
        # Extract recent context (last 2 turns = 4 messages)
        recent_turns = history[-4:] if len(history) >= 4 else history
        
        if not recent_turns:
            return user_query
        
        # Build context string (truncate long messages)
        context_lines = []
        for msg in recent_turns:
            role = "ç”¨æˆ·" if msg['role'] == 'user' else "åŠ©æ‰‹"
            content = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
            context_lines.append(f"{role}: {content}")
        context_str = "\n".join(context_lines)
        
        # Construct rewrite prompt
        rewrite_prompt = f"""åŸºäºŽä»¥ä¸‹å¯¹è¯åŽ†å²ï¼Œå°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªç‹¬ç«‹å®Œæ•´çš„æ£€ç´¢æŸ¥è¯¢ã€‚

ã€é‡è¦è§„åˆ™ã€‘
1. **æŒ‡ä»£æ¶ˆé™¤**ï¼šå¦‚æžœç”¨æˆ·è¯´â€œæ ¹æ®ä¸Šä¸‹æ–‡â€ã€â€œæ ¹æ®åŽ†å²â€ã€â€œå®ƒâ€ã€â€œè¿™ä¸ªâ€ï¼Œè¯·åŠ¡å¿…æŠŠè¿™äº›æŒ‡ä»£è¯æ›¿æ¢ä¸ºå¯¹è¯åŽ†å²ä¸­å…·ä½“è®¨è®ºçš„**æ ¸å¿ƒä¸»é¢˜**ã€‚
2. **åŒºåˆ†â€œåŽ†å²è®°å½•â€ä¸Žâ€œåŽ†å²å­¦ç§‘â€**ï¼š
   - å¦‚æžœç”¨æˆ·è¯´â€œæ ¹æ®åŽ†å²ç”Ÿæˆ...â€ï¼Œé€šå¸¸æ˜¯æŒ‡â€œæ ¹æ®å¯¹è¯åŽ†å²ä¸­çš„å†…å®¹â€ï¼Œè¯·æå–å¯¹è¯ä¸­çš„ä¸»é¢˜ï¼ˆå¦‚â€œé«˜è´¨é‡å‘å±•â€ï¼‰ã€‚
   - åªæœ‰å½“ç”¨æˆ·æ˜Žç¡®æåˆ°â€œåŽ†å²äº‹ä»¶â€ã€â€œå¤ä»£â€ç­‰è¯æ—¶ï¼Œæ‰ä¿ç•™â€œåŽ†å²â€ä½œä¸ºå­¦ç§‘å…³é”®è¯ã€‚
3. **ä¿æŒåŽŸæ„**ï¼šä¸è¦éšæ„æ‰©å±•æ— å…³å†…å®¹ã€‚

å¯¹è¯åŽ†å²ï¼š
{context_str}

ç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š{user_query}

æ”¹å†™åŽçš„ç‹¬ç«‹æŸ¥è¯¢ï¼ˆä»…è¾“å‡ºæ”¹å†™åŽçš„å¥å­ï¼‰ï¼š"""
        try:
            print(f"ðŸ”„ Query rewriting: {user_query}")
            
            # Use lightweight model for fast rewriting
            completion = self.client.chat.completions.create(
                model=self.lite_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æŸ¥è¯¢æ”¹å†™ä¸“å®¶ã€‚å°†ä¸å®Œæ•´çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹å®Œæ•´çš„æ£€ç´¢æŸ¥è¯¢ã€‚è¯·æ³¨æ„è¦åŒæ—¶é€å­—ä¿ç•™ä¸€ä»½ç”¨æˆ·åŽŸæœ¬çš„æŒ‡ä»¤å¹¶æ ‡æ³¨è¿™æ˜¯åŽŸå§‹æŒ‡ä»¤ï¼ŒåŽŸå§‹æŒ‡ä»¤ä¼šè¢«ä½¿ç”¨æ¥è¿›è¡Œå…¨å±€çš„ç²¾å‡†åŒ¹é…ã€‚"},
                    {"role": "user", "content": rewrite_prompt}
                ],
                max_tokens=100,  # Limit output length
                temperature=0.2   # Low temperature for consistent rewrites
            )
            
            rewritten = completion.choices[0].message.content.strip()
            print(f"âœ… Rewritten query: {rewritten}")
            return rewritten
            
        except Exception as e:
            print(f"âŒ Query rewrite failed: {e}, using original query")
            return user_query
    
    def _should_rewrite(self, query: str, history: List[dict]) -> bool:
        """
        Determine if query needs rewriting based on simple heuristics.
        
        Returns:
            True if query appears to depend on context
        """
        if not history:
            return False
        
        # Indicators that query is incomplete or context-dependent
        indicators = [
            len(query) < 15,  # Very short query
            any(word in query for word in ["å®ƒ", "è¿™ä¸ª", "é‚£ä¸ª", "å‘¢", "è¿˜æœ‰", "è¿™", "é‚£"]),  # Pronouns/references
            query.strip().endswith("ï¼Ÿ") and len(query) < 10,  # Short question
            any(phrase in query for phrase in ["è¯¦ç»†", "ä¸¾ä¾‹", "ä¸ºä»€ä¹ˆ", "æ€Žä¹ˆ", "å¦‚ä½•"])  # Follow-up keywords
        ]
        
        return any(indicators)

