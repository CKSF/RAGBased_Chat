from flask import Blueprint, request, jsonify
from backend.app.services import rag_service, llm_service

chat_bp = Blueprint('chat', __name__)

@chat_bp.route('/send', methods=['POST'])
def send_message():
    """
    Handle chat messages.
    Payload: { "message": "ä»€ä¹ˆæ˜¯æ€æ”¿è¯¾?", "history": [...] }
    """
    data = request.json
    if not data or 'message' not in data:
        return jsonify({"error": "Message is required"}), 400
        
    user_message = data['message']
    history = data.get('history', [])
    
    try:
        # 1. Sliding Window: Truncate history to last 5 turns (10 messages)
        MAX_HISTORY_TURNS = 5
        if len(history) > MAX_HISTORY_TURNS * 2:
            history = history[-(MAX_HISTORY_TURNS * 2):]
            print(f"ğŸ“ Truncated history to last {MAX_HISTORY_TURNS} turns")
        
        # 2. Context-Aware Query Rewriting
        rewritten_query = llm_service.rewrite_query(user_message, history)
        
        # 3. RAG Retrieval (use rewritten query)
        print(f"ğŸ” RAG query: '{rewritten_query}'", flush=True)
        documents = rag_service.query(rewritten_query, k=3)
        print(f"DEBUG: RAG query complete. Found {len(documents)} docs", flush=True)

        context_text = ""
        sources = []
        for doc in documents:
            source_name = doc.metadata.get('source', 'Unknown')
            if source_name not in sources:
                sources.append(source_name)
            context_text += f"\n---\n[Source: {source_name}]\n{doc.page_content}\n"

        # 2. Prompt
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€æ”¿è¯¾åŠ©æ•™å¤§æ¨¡å‹ã€‚è¯·æ ¹æ®æä¾›çš„èƒŒæ™¯èµ„æ–™ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
            "å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜ï¼Œå¹¶å°è¯•ç”¨ä½ çš„é€šç”¨çŸ¥è¯†è¡¥å……ï¼Œä½†è¦æ˜ç¡®åŒºåˆ†ã€‚\n"
            "å›ç­”é£æ ¼ï¼šä¸¥è°¨ã€å‡†ç¡®ã€ç§¯ææ­£å‘ã€‚\n\n"
            f"### èƒŒæ™¯èµ„æ–™ (Context):\n{context_text}"
        )
        
        # 3. LLM
        print(f"DEBUG: Preparing to call LLM...", flush=True)
        response_text = llm_service.get_response(
            user_message, 
            system_prompt=system_prompt,
            history=history
        )
        print(f"DEBUG: LLM returned response length: {len(response_text)}", flush=True)
        
        return jsonify({
            "reply": response_text,
            "sources": sources,
            "context_used": context_text[:200] + "..." 
        })

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        
        # Log to file
        with open("backend_errors.log", "a", encoding="utf-8") as f:
            f.write(f"\n[{request.remote_addr}] Chat Error:\n{error_trace}\n{'='*50}\n")
            
        print(f"âŒ Chat Error:\n{error_trace}", flush=True)
        return jsonify({"error": f"{str(e)}\n\nTraceback:\n{error_trace}"}), 500
