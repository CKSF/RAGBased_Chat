import json
import traceback
from flask import Blueprint, request, jsonify, stream_with_context, Response
from backend.app.services import rag_service, llm_service

chat_bp = Blueprint('chat', __name__)

def format_sse(event_type: str, data: dict):
    """Helper to format Server-Sent Events (SSE)."""
    return f"data: {json.dumps({'type': event_type, 'data': data}, ensure_ascii=False)}\n\n"

@chat_bp.route('/send', methods=['POST'])
def send_message():
    # 1. Parse Request
    req_data = request.json
    user_message = req_data.get('message', '')
    history = req_data.get('history', [])

    def generate():
        try:
            # --- STEP 1: QUERY REWRITING ---
            yield format_sse('thought', "ğŸ¤” æ­£åœ¨ç†è§£æ‚¨çš„é—®é¢˜ä¸Šä¸‹æ–‡...")
            rewritten_query = llm_service.rewrite_query(user_message, history)
            
            if rewritten_query != user_message:
                yield format_sse('thought', f"ğŸ”„ ä¼˜åŒ–æŸ¥è¯¢ä¸º: â€œ{rewritten_query}â€")

            # --- STEP 2: RAG RETRIEVAL ---
            yield format_sse('thought', "ğŸ“š æ­£åœ¨æ£€ç´¢æ€æ”¿çŸ¥è¯†åº“...")
            
            # Perform Query
            documents = rag_service.query(rewritten_query, k=3)
            
            # --- STEP 3: INTERMEDIATE DATA (THE COLLAPSIBLE INFO) ---
            doc_count = len(documents)
            if doc_count == 0:
                yield format_sse('thought', "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†å›ç­”ã€‚")
                context_text = ""
                sources = []
            else:
                sources = list(set([doc.metadata.get('source', 'Unknown') for doc in documents]))
                yield format_sse('thought', f"âœ… æ£€ç´¢å®Œæˆï¼šæ‰¾åˆ° {doc_count} ä»½ç›¸å…³æ–‡æ¡£ã€‚")
                yield format_sse('thought', f"ğŸ“„ å‚è€ƒæ¥æº: {', '.join(sources)}")
                
                context_text = ""
                for doc in documents:
                    context_text += f"\n---\n[Source: {doc.metadata.get('source')}]\n{doc.page_content}\n"

            # --- STEP 4: LLM GENERATION ---
            yield format_sse('thought', "ğŸ§  æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")
            
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€æ”¿è¯¾åŠ©æ•™å¤§æ¨¡å‹ã€‚è¯·æ ¹æ®æä¾›çš„èƒŒæ™¯èµ„æ–™ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
                "å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜ï¼Œå¹¶å°è¯•ç”¨ä½ çš„é€šç”¨çŸ¥è¯†è¡¥å……ï¼Œä½†è¦æ˜ç¡®åŒºåˆ†ã€‚\n"
                "å›ç­”é£æ ¼ï¼šä¸¥è°¨ã€å‡†ç¡®ã€ç§¯ææ­£å‘ã€‚\n\n"
                f"### èƒŒæ™¯èµ„æ–™ (Context):\n{context_text}"
            )

            # Stream the actual text token-by-token
            for token in llm_service.stream_response(user_message, system_prompt, history):
                yield format_sse('token', token)
            
            # --- STEP 5: FINISH ---
            # Send the sources one last time so the UI can lock them in
            yield format_sse('done', {"sources": sources})

        except Exception as e:
            traceback.print_exc()
            yield format_sse('error', str(e))

    return Response(stream_with_context(generate()), mimetype='text/event-stream')
