import json
import traceback
from flask import Blueprint, request, jsonify, stream_with_context, Response
from backend.app.services import rag_service, llm_service

chat_bp = Blueprint('chat', __name__)

def format_sse(event_type: str, data: dict):
    """Helper to format Server-Sent Events (SSE)."""
    return f"data: {json.dumps({'type': event_type, 'data': data}, ensure_ascii=False)}\n\n"

@chat_bp.route('/send', methods=['POST'])
def send_message():
    # 1. Parse Request
    req_data = request.json
    user_message = req_data.get('message', '')
    history = req_data.get('history', [])
    grade = req_data.get('grade', 'é€šç”¨')

    # --- FILTER LOGIC (Copied from Lesson BP) ---
    rag_filters = {}
    if grade and grade != 'é€šç”¨':
        if "å°å­¦" in grade:
            rag_filters = {"grade": "å°å­¦"}
        elif "åˆä¸­" in grade:
            rag_filters = {"grade": "åˆä¸­"}
        elif "é«˜ä¸­" in grade:
            rag_filters = {"grade": "é«˜ä¸­"}
        elif "å¤§å­¦" in grade or "æœ¬ç§‘" in grade:
            rag_filters = {"grade": "å¤§å­¦"}
        elif "ç¡•å£«" in grade or "ç ”ç©¶ç”Ÿ" in grade:
            rag_filters = {"grade": "ç¡•å£«"}
        elif "åšå£«" in grade:
            rag_filters = {"grade": "åšå£«"}

    def generate():
        try:
            # --- STEP 1: QUERY REWRITING ---
            # We append the grade to the query context so the rewriter knows the level
            rewrite_context_msg = f"{user_message} (Target Audience: {grade})"
            
            yield format_sse('thought', "ğŸ¤” æ­£åœ¨ç†è§£æ‚¨çš„é—®é¢˜ä¸Šä¸‹æ–‡...")
            rewritten_query = llm_service.rewrite_query(rewrite_context_msg, history)
            
            # If rewrite failed/skipped, ensure we still search for the user message
            final_query = rewritten_query if rewritten_query else user_message

            if final_query != user_message:
                yield format_sse('thought', f"ğŸ”„ ä¼˜åŒ–æŸ¥è¯¢ä¸º: â€œ{final_query}â€")

            # --- STEP 2: RAG RETRIEVAL ---
            yield format_sse('thought', f"ğŸ“š æ­£åœ¨æ£€ç´¢æ€æ”¿çŸ¥è¯†åº“ (Filter: {rag_filters})...")
            
            # Perform Query with FILTERS
            documents = rag_service.query(final_query, k=8, filters=rag_filters)
            
            # --- STEP 3: INTERMEDIATE DATA ---
            doc_count = len(documents)
            if doc_count == 0:
                yield format_sse('thought', "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†å›ç­”ã€‚")
                context_text = ""
                sources = []
            else:
                sources = list(set([doc.metadata.get('source', 'Unknown') for doc in documents]))
                yield format_sse('thought', f"âœ… æ£€ç´¢å®Œæˆï¼šæ‰¾åˆ° {doc_count} ä»½ç›¸å…³æ–‡æ¡£ã€‚")
                yield format_sse('thought', f"ğŸ“„ å‚è€ƒæ¥æº: {', '.join(sources)}")
                
                # --- GROUP DOCUMENTS BY SOURCE ---
                # Key: source_name, Value: list of docs
                grouped_docs = {}
                for doc in documents:
                    src = doc.metadata.get('source', 'Unknown')
                    if src not in grouped_docs:
                        grouped_docs[src] = []
                    grouped_docs[src].append(doc)
                
                context_text = ""
                # Iterate through unique sources (giving each a Book ID)
                for i, (source_name, docs) in enumerate(grouped_docs.items()):
                    # Take metadata from first doc of the group
                    first_meta = docs[0].metadata
                    doc_grade = first_meta.get('grade', 'é€šç”¨')
                    
                    # Header for the Book
                    context_text += f"\nã€èµ„æ–™ {i+1}ã€‘ã€Š{source_name}ã€‹ (é€‚ç”¨: {doc_grade})\n"
                    
                    # List content for each page/chunk in this book
                    for sub_doc in docs:
                        sub_page = sub_doc.metadata.get('page', '?')
                        # Format: [ç¬¬ 5 é¡µ]: Content
                        context_text += f"   - [ç¬¬ {sub_page} é¡µ]: {sub_doc.page_content}\n"
                    
                    context_text += "\n"

            # --- STEP 4: LLM GENERATION ---
            print(f"\n[DEBUG] Context Sent to LLM:\n{context_text}\n[END DEBUG]\n")
            yield format_sse('thought', "ğŸ§  æ­£åœ¨è¿›è¡Œäº‹å®æ ¸æŸ¥ä¸ç­”æ¡ˆç”Ÿæˆ...")
            
            
            system_prompt = (
    "ä½ æ˜¯ä¸€ä¸ªæå…¶ä¸¥è°¨çš„æ€æ”¿è¯¾åŠ©æ•™å¤§æ¨¡å‹ï¼ˆä¸“æ³¨é—®ç­”ä¸æ¦‚å¿µè§£æï¼‰ã€‚\n"
    "ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åŸºäºã€èƒŒæ™¯èµ„æ–™ã€‘ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›æ¸…æ™°ã€æµç•…çš„è§£é‡Šã€‚\n\n"
    "### âš ï¸ èº«ä»½ä¸æ ¼å¼ä¸¥æ ¼é™åˆ¶ (Identity & Format Constraints)\n"
    "1. **è§’è‰²å®šä¹‰**ï¼šä½ ç°åœ¨æ˜¯**é—®ç­”åŠ©æ‰‹**ï¼Œä¸æ˜¯æ•™æ¡ˆç”Ÿæˆå™¨ã€‚\n"
    "2. **ä¸¥ç¦è¾“å‡ºæ•™æ¡ˆç»“æ„**ï¼š**ç»å¯¹ç¦æ­¢**ä½¿ç”¨â€œæ•™å­¦ç›®æ ‡â€ã€â€œæ•™å­¦é‡éš¾ç‚¹â€ã€â€œæ•™å­¦è¿‡ç¨‹â€ã€â€œæ¿ä¹¦è®¾è®¡â€ã€â€œè¯¾åä½œä¸šâ€ç­‰æ•™æ¡ˆä¸“ç”¨æ ¼å¼ã€‚\n"
    "3. **æ–‡é£è¦æ±‚**ï¼šè¯·ä½¿ç”¨é€šé¡ºçš„æ®µè½è¿›è¡Œè¯´æ˜ï¼ˆExplanatory Paragraphsï¼‰ã€‚ä¸è¦ç½—åˆ—è¿‡å¤šçš„çŸ¥è¯†ç‚¹å¤§çº²ï¼Œè€Œæ˜¯è¦æŠŠé€»è¾‘è®²æ¸…æ¥šã€‚\n\n"
    "ä¸ºäº†ç¡®ä¿ä¿¡æ¯çš„ç»å¯¹å‡†ç¡®æ€§ï¼Œä½ å¿…é¡»**ä¸¥æ ¼åŒºåˆ†**â€œèµ„æ–™åº“å†…å®¹â€ä¸â€œé€šç”¨æ¨ç†å†…å®¹â€ã€‚\n\n"
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·å…ˆåˆ¤æ–­æä¾›çš„ã€èƒŒæ™¯èµ„æ–™ã€‘æ˜¯å¦èƒ½å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\n\n"
    "**æƒ…å†µä¸€ï¼šèµ„æ–™é«˜åº¦ç›¸å…³**\n"
    "- å¿…é¡»**ä¼˜å…ˆ**å¼•ç”¨èµ„æ–™ä¸­çš„åŸæ–‡ã€è§‚ç‚¹æˆ–æ¡ˆä¾‹ã€‚\n"
    "- ä¸¥ç¦è„±ç¦»èµ„æ–™è¿›è¡Œä¸å¿…è¦çš„å‘æŒ¥ã€‚\n\n"
    "**æƒ…å†µäºŒï¼šèµ„æ–™ä¸ç›¸å…³ã€ç›¸å…³åº¦ä½ æˆ– æ— æ³•å›ç­”æ ¸å¿ƒé—®é¢˜**\n"
    "- **å¿…é¡»**åœ¨å›ç­”çš„æœ€å¼€å§‹ï¼Œ**åŠ ç²—**è¾“å‡ºä»¥ä¸‹å…è´£å£°æ˜ï¼š\n"
    "  > **ã€æ³¨ï¼šçŸ¥è¯†åº“ä¸­æœªåŒ¹é…åˆ°é«˜ç›¸å…³åº¦èµ„æ–™ï¼Œä»¥ä¸‹å†…å®¹åŸºäºé€šç”¨çŸ¥è¯†è¡¥å……ï¼Œä»…ä¾›å‚è€ƒã€‚ã€‘**\n"
    "### æ ¸å¿ƒé“å¾‹ï¼ˆè¿åå³è§†ä¸ºä¸¥é‡é”™è¯¯ï¼‰ï¼š\n"
    "1. **ä¼˜å…ˆåŸæ–‡**ï¼šå›ç­”å¿…é¡»åŸºäº èƒŒæ™¯èµ„æ–™(Context) å†…å®¹ã€‚å¦‚æœ èƒŒæ™¯èµ„æ–™(Context) åŒ…å«ç¡®åˆ‡ç­”æ¡ˆï¼Œè¯·ç›´æ¥å¼•ç”¨ã€‚\n"
    "2. **ä¸¥ç¦ç¼–é€ **ï¼šå¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œå¿…é¡»è¯šå®è¯´æ˜ã€‚å¦‚æœç”¨æˆ·é—®â€œçº¢å†›é•¿å¾â€ï¼Œè€Œèµ„æ–™é‡Œå…¨æ˜¯â€œæ”¹é©å¼€æ”¾â€ï¼Œè¯·ç›´æ¥æŒ‰ã€æƒ…å†µäºŒã€‘å¤„ç†ï¼Œç»å¯¹ä¸è¦å¼ºè¡ŒæŠŠæ”¹é©å¼€æ”¾çš„å†…å®¹å¥—ç”¨åˆ°é•¿å¾ä¸Šã€‚å¦‚æœä¸ç¡®å®šï¼Œå°±è¯´ä¸çŸ¥é“ã€‚\n"
    "3. **å¼ºåˆ¶ç´¢å¼•å¼•ç”¨ (Strict Indexing)**ï¼šè¿™æ˜¯æœ€é‡è¦çš„è§„åˆ™ã€‚\n"
    "   - å½“ä½ å¼•ç”¨èƒŒæ™¯èµ„æ–™ä¸­çš„å†…å®¹æ—¶ï¼Œ**å¿…é¡»**åœ¨å¥æœ«æ ‡æ³¨æ¥æºç´¢å¼•ï¼Œæ ¼å¼ä¸º `[1]` æˆ– `[2]`ã€‚\n"
    "3. **å¼ºåˆ¶ç´¢å¼•å¼•ç”¨ (Strict Indexing)**ï¼š\n"
    "   - åœ¨æ­£æ–‡å›ç­”ä¸­ï¼Œè¯·ä½¿ç”¨ **`ã€Šæ–‡ä»¶åã€‹[ID](ç¬¬Xé¡µ)`** æˆ–è€… **ç®€å•çš„ `[ID]`** (å¦‚æœè¯¥IDåœ¨ç¬¬ä¸€éƒ¨åˆ†Evidence Baseå·²æ˜ç¡®åˆ—å‡º)ã€‚\n"
    "   - æ¨èåœ¨æ­£æ–‡ç»¼åˆåˆ†ææ—¶ï¼Œä½¿ç”¨ç®€å•çš„ `[ID]` æ¥ä¿æŒæµç•…åº¦ï¼Œä½†ç¬¬ä¸€æ¬¡å¼•ç”¨æŸä¹¦æ—¶æœ€å¥½å¸¦ä¸Šä¹¦åã€‚\n"
    
    "### ğŸ–Šï¸ è¾“å‡ºç»“æ„å¼ºåˆ¶è¦æ±‚ (å¿…é¡»å®Œå…¨éµå®ˆ):\n"
    "è¯·å°†ä½ çš„å›ç­”ä¸¥æ ¼åˆ’åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š\n\n"
    
    "#### ç¬¬ä¸€éƒ¨åˆ†ï¼šğŸ“š çŸ¥è¯†åº“ç²¾å‡†ä¾æ® (Evidence Base)\n"
    "**å¿…é¡»æŒ‰æ¥æºä¹¦ç±åˆ†ç»„å±•ç¤º**ï¼Œæ ¼å¼å¦‚ä¸‹ (ä¸¥ç¦é”™ä¹±)ï¼š\n"
    "**ã€Šä¹¦ç±åç§° Aã€‹ã€1ã€‘**:\n"
    "1. \"...å¼•ç”¨åŸæ–‡ç‰‡æ®µ...\" (ç¬¬ 5 é¡µ)\n"
    "2. \"...å¼•ç”¨åŸæ–‡ç‰‡æ®µ...\" (ç¬¬ 12 é¡µ)\n\n"
    "**ã€Šä¹¦ç±åç§° Bã€‹ã€2ã€‘**:\n"
    "1. \"...å¼•ç”¨åŸæ–‡ç‰‡æ®µ...\" (ç¬¬ 8 é¡µ)\n\n"
    "- å¿…é¡»å®Œå…¨åŸºäºæä¾›çš„ã€èµ„æ–™ 1ã€‘ã€ã€èµ„æ–™ 2ã€‘ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆæ­¤éƒ¨åˆ†ã€‚\n"
    "- æ¯ä¸€æ¡å¼•ç”¨å¿…é¡»ç²¾ç¡®åˆ°é¡µç ã€‚\n\n"
    
    "#### ç¬¬äºŒéƒ¨åˆ†ï¼šğŸ’¡ ç»¼åˆè§£ç­”ä¸åŠ©æ•™è§£æ (Analysis & Answer)\n"
    "- åœ¨æ­¤éƒ¨åˆ†ï¼ŒåŸºäºç¬¬ä¸€éƒ¨åˆ†çš„è¯æ®ï¼Œç»“åˆä½ çš„æ•™å­¦é€»è¾‘ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
    "- **ä½¿ç”¨å¼•ç”¨ç´¢å¼•**ï¼šå½“ç”¨åˆ°ä¸Šè¿°è¯æ®æ—¶ï¼Œä½¿ç”¨ `ã€1ã€‘` æˆ– `ã€2ã€‘` æ ‡æ³¨ã€‚\n"
    "- **å¼•ç”¨ä½ç½®**ï¼šå¼•ç”¨ç¼–å·å¿…é¡»æ”¾åœ¨**å¥å·ä¹‹å**ã€‚\n"
    "  - âœ… æ­£ç¡®ï¼š...ä¿ƒè¿›äººçš„å…¨é¢å‘å±•ã€‚ã€1ã€‘\n"
    "  - âŒ é”™è¯¯ï¼š...ä¿ƒè¿›äººçš„å…¨é¢å‘å±•[1]ã€‚\n"
    "- è¯­è¨€é£æ ¼ï¼šä¸¥è°¨ã€ç§¯æã€ç¬¦åˆç›®æ ‡å¹´çº§è®¤çŸ¥ã€‚\n"
    "### æ ¼å¼å¼ºåˆ¶è§„èŒƒ (Critical Output Rules)ï¼š\n"
    "ç”±äºå‰ç«¯æ˜¾ç¤ºé™åˆ¶ï¼Œ**ç³»ç»Ÿæ— æ³•æ¸²æŸ“ä»»ä½•å›¾è¡¨ä»£ç **ã€‚ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š\n"
    "1. **ç»“æ„åŒ–é™çº§å¤„ç†**ï¼š\n"
    "   - å½“ä½ æƒ³ç”»â€œæµç¨‹å›¾â€æ—¶ï¼Œ**å¿…é¡»**æ”¹å†™ä¸ºã€å¸¦åºå·çš„æ­¥éª¤åˆ—è¡¨ã€‘(1. -> 2. -> 3.)ã€‚\n"
    "   - å½“ä½ æƒ³ç”»â€œæ€ç»´å¯¼å›¾â€æˆ–â€œå±‚çº§ç»“æ„â€æ—¶ï¼Œ**å¿…é¡»**æ”¹å†™ä¸ºã€å¤šçº§ç¼©è¿›åˆ—è¡¨ã€‘(- ç¬¬ä¸€å±‚ \n  - ç¬¬äºŒå±‚)ã€‚\n"
    "   - å½“ä½ æƒ³ç”»â€œé¥¼å›¾â€æˆ–â€œè¡¨æ ¼â€æ—¶ï¼Œ**å¿…é¡»**ä½¿ç”¨æ ‡å‡†çš„ Markdown è¡¨æ ¼ (| header | ...)ã€‚\n"
    "2. **ç»å¯¹ç¦æ­¢**ï¼š\n"
    "   - ä¸¥ç¦è¾“å‡ºä»»ä½• ```mermaid, ```flowchart, ```graph, ```pie ç­‰ä»£ç å—ã€‚\n"
    "   - ä¸¥ç¦è¾“å‡º `<svg>` æ ‡ç­¾ã€‚\n"
    "   - è¾“å‡ºå†…å®¹å¿…é¡»æ˜¯ç›´æ¥å¯è¯»çš„çº¯æ–‡æœ¬ Markdownã€‚"
    f"\n\n### èƒŒæ™¯èµ„æ–™ (Indexed Context):\n{context_text}"
)

            # Stream the actual text token-by-token
            for token in llm_service.stream_response(user_message, system_prompt, history):
                yield format_sse('token', token)
            
            # Construct Rich Citations
            rich_citations = []
            if documents:
                 for doc in documents:
                    meta = doc.metadata
                    rich_citations.append({
                        "source": meta.get('source', 'Unknown'),
                        "page": meta.get('page', '?'),
                        "grade": meta.get('grade', 'é€šç”¨'),
                        "content": doc.page_content[:300] + "..." # Snippet for preview
                    })

            yield format_sse('done', {"sources": rich_citations})

        except Exception as e:
            traceback.print_exc()
            yield format_sse('error', str(e))

    return Response(stream_with_context(generate()), mimetype='text/event-stream')