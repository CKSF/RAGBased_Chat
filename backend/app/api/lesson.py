import json
import traceback
from flask import Blueprint, request, Response, stream_with_context
from backend.app.services import rag_service, llm_service

lesson_bp = Blueprint('lesson', __name__)

def format_sse(event_type: str, data: dict):
    """Helper to format Server-Sent Events."""
    return f"data: {json.dumps({'type': event_type, 'data': data}, ensure_ascii=False)}\n\n"

@lesson_bp.route('/generate', methods=['POST'])
def generate_lesson_plan():
    """
    Generate a lesson plan via Streaming.
    Payload: { "topic": "é«˜è´¨é‡å‘å±•", "grade": "å°å­¦" }
    """
    req_data = request.json
    topic = req_data.get('topic', '')
    grade = req_data.get('grade', 'ä¸é™')

    def generate():
        try:
            # --- STEP 1: ANALYSIS ---
            yield format_sse('thought', f"ğŸ¯ æ­£åœ¨è§£ææ•™å­¦éœ€æ±‚ï¼šã€{grade}ã€‘{topic}...")
            
            # --- STEP 2: RAG SEARCH ---
            query = f"{grade} {topic}"
            yield format_sse('thought', f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³æ€æ”¿è¯¾æ ‡ä¸ç´ æ: '{query}'...")
            
            # Retrieve 5 docs (Standard search)
            documents = rag_service.query(query, k=8)
            
            if not documents:
                yield format_sse('thought', "âš ï¸ æœªæ‰¾åˆ°ç‰¹å®šç´ æï¼Œå°†åŸºäºé€šç”¨æ•™å­¦ç†è®ºè®¾è®¡ã€‚")
                context_text = ""
                sources = []
            else:
                sources = list(set([doc.metadata.get('source', 'Unknown') for doc in documents]))
                yield format_sse('thought', f"âœ… æ‰¾åˆ° {len(documents)} ä»½å‚è€ƒèµ„æ–™ï¼Œæ­£åœ¨æå–æ ¸å¿ƒè§‚ç‚¹...")
                yield format_sse('thought', f"ğŸ“„ å‚è€ƒæ¥æº: {', '.join(sources)}")
                
                context_text = ""
                for doc in documents:
                    context_text += f"\n---\n[Source: {doc.metadata.get('source')}]\n{doc.page_content}\n"

            # --- STEP 3: PROMPT CONSTRUCTION ---
            yield format_sse('thought', "ğŸ—ï¸ æ­£åœ¨æ„å»ºæ•™å­¦ç›®æ ‡ã€é‡éš¾ç‚¹ä¸äº’åŠ¨ç¯èŠ‚...")
            
            system_prompt = (
                "ä½ æ˜¯ä¸€åèµ„æ·±çš„å¤§ä¸­å°å­¦æ€æ”¿è¯¾éª¨å¹²æ•™å¸ˆã€‚\n"
                "ä»»åŠ¡ï¼šè¯·åŸºäºæä¾›çš„èƒŒæ™¯èµ„æ–™ï¼Œä¸ºä¸€èŠ‚45åˆ†é’Ÿçš„æ€æ”¿è¯¾è®¾è®¡ä¸€ä»½è¯¦ç»†çš„æ•™æ¡ˆã€‚\n"
                "è¦æ±‚ï¼š\n"
                "1. ç»“æ„å®Œæ•´ï¼ˆæ•™å­¦ç›®æ ‡ã€é‡éš¾ç‚¹ã€æ•™å­¦æ–¹æ³•ã€æ•™å­¦è¿‡ç¨‹ã€æ¿ä¹¦è®¾è®¡ï¼‰ã€‚\n"
                "2. æ•™å­¦è¿‡ç¨‹è¦è®¾è®¡å…·ä½“çš„äº’åŠ¨ç¯èŠ‚ï¼ˆå¦‚æé—®ã€è®¨è®ºï¼‰ã€‚\n"
                "3. å¿…é¡»å……åˆ†èåˆèƒŒæ™¯èµ„æ–™ä¸­çš„æ ¸å¿ƒè§‚ç‚¹å’Œæ¡ˆä¾‹ã€‚\n"
                "4. è¯­è¨€ç”ŸåŠ¨ï¼Œç¬¦åˆå­¦ç”Ÿè®¤çŸ¥æ°´å¹³ã€‚\n"
                "5. æ ¼å¼ä½¿ç”¨ Markdownï¼Œæ ‡é¢˜æ¸…æ™°ã€‚\n\n"
                f"### èƒŒæ™¯èµ„æ–™:\n{context_text}"
            )
            
            user_prompt = f"è¯·ä»¥ã€Š{topic}ã€‹ä¸ºä¸»é¢˜ï¼Œè®¾è®¡ä¸€ä»½é’ˆå¯¹ã€{grade}ã€‘å­¦ç”Ÿçš„æ•™æ¡ˆã€‚"

            # --- STEP 4: LLM STREAMING ---
            yield format_sse('thought', "âœï¸ å¼€å§‹æ’°å†™æ•™æ¡ˆ...")
            
            # Reusing the existing stream_response method from llm_service
            for token in llm_service.stream_response(user_prompt, system_prompt, history=[]):
                yield format_sse('token', token)
            
            # --- STEP 5: FINISH ---
            yield format_sse('done', {"sources": sources})

        except Exception as e:
            traceback.print_exc()
            yield format_sse('error', str(e))

    return Response(stream_with_context(generate()), mimetype='text/event-stream')