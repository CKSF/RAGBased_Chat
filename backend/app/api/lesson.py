from flask import Blueprint, request, jsonify
from backend.app.services import rag_service, llm_service

lesson_bp = Blueprint('lesson', __name__)

@lesson_bp.route('/generate', methods=['POST'])
def generate_lesson_plan():
    """
    Generate a lesson plan.
    Payload: { "topic": "é«˜è´¨é‡å‘å±•", "grade": "å°å­¦" }
    """
    data = request.json
    if not data or 'topic' not in data:
        return jsonify({"error": "Topic is required"}), 400
        
    topic = data['topic']
    grade = data.get('grade', 'ä¸é™')
    
    try:
        # 1. RAG Retrieval (Deeper search)
        # We search for the topic + grade to get specific info
        query = f"{grade} {topic}"
        print(f"ğŸ” Searching abundant context for lesson plan: {query}")
        
        # Retrieve more docs for lesson planning (e.g., k=6)
        # Since our query method in RAGService uses ParentDocumentRetriever via .invoke(),
        # passing 'k' might not be directly supported unless we modified query logic.
        # But let's rely on standard search for now.
        documents = rag_service.query(query, k=5)
        
        context_text = ""
        sources = []
        for doc in documents:
            src = doc.metadata.get('source', 'Unknown')
            if src not in sources: sources.append(src)
            context_text += f"\n---\n{doc.page_content}\n"

        # 2. Construct Prompt
        # DeepSeek-R1 (Reasoning) is excellent at structured tasks.
        system_prompt = (
            "ä½ æ˜¯ä¸€åèµ„æ·±çš„å¤§ä¸­å°å­¦æ€æ”¿è¯¾éª¨å¹²æ•™å¸ˆã€‚\n"
            "ä»»åŠ¡ï¼šè¯·åŸºäºæä¾›çš„èƒŒæ™¯èµ„æ–™ï¼Œä¸ºä¸€èŠ‚45åˆ†é’Ÿçš„æ€æ”¿è¯¾è®¾è®¡ä¸€ä»½è¯¦ç»†çš„æ•™æ¡ˆã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. ç»“æ„å®Œæ•´ï¼ˆæ•™å­¦ç›®æ ‡ã€é‡éš¾ç‚¹ã€æ•™å­¦æ–¹æ³•ã€æ•™å­¦è¿‡ç¨‹ã€æ¿ä¹¦è®¾è®¡ï¼‰ã€‚\n"
            "2. æ•™å­¦è¿‡ç¨‹è¦è®¾è®¡å…·ä½“çš„äº’åŠ¨ç¯èŠ‚ï¼ˆå¦‚æé—®ã€è®¨è®ºï¼‰ã€‚\n"
            "3. å¿…é¡»å……åˆ†èåˆèƒŒæ™¯èµ„æ–™ä¸­çš„æ ¸å¿ƒè§‚ç‚¹å’Œæ¡ˆä¾‹ã€‚\n"
            "4. è¯­è¨€ç”ŸåŠ¨ï¼Œç¬¦åˆå­¦ç”Ÿè®¤çŸ¥æ°´å¹³ã€‚\n\n"
            f"### èƒŒæ™¯èµ„æ–™:\n{context_text}"
        )
        
        user_prompt = f"è¯·ä»¥ã€Š{topic}ã€‹ä¸ºä¸»é¢˜ï¼Œè®¾è®¡ä¸€ä»½é’ˆå¯¹ã€{grade}ã€‘å­¦ç”Ÿçš„æ•™æ¡ˆã€‚"
        
        # 3. Call LLM
        response_text = llm_service.get_response(
            user_prompt,
            system_prompt=system_prompt
        )
        
        return jsonify({
            "lesson_plan": response_text,
            "sources": sources
        })

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        
        # Log to file
        with open("backend_errors.log", "a", encoding="utf-8") as f:
            f.write(f"\n[{request.remote_addr}] Lesson Error:\n{error_trace}\n{'='*50}\n")

        print(f"âŒ Lesson Plan Error:\n{error_trace}", flush=True)
        return jsonify({"error": f"{str(e)}\n\nTraceback:\n{error_trace}"}), 500
